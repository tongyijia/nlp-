{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data as Data\n",
    "\n",
    "import d2lzh_pytorch as d2l\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 处理数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# sentences: 42068'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert 'ptb.train.txt' in os.listdir(\"./data/\")\n",
    "\n",
    "with open('./data/ptb.train.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    raw_dataset = [st.split() for st in lines]\n",
    "\n",
    "'# sentences: %d' % len(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#tokens: 24 ['aer', 'banknote', 'berlitz', 'calloway', 'centrust']\n",
      "#tokens: 15 ['pierre', '<unk>', 'N', 'years', 'old']\n",
      "#tokens: 11 ['mr.', '<unk>', 'is', 'chairman', 'of']\n"
     ]
    }
   ],
   "source": [
    "for st in raw_dataset[:3]:\n",
    "    print('#tokens:', len(st), st[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立词语索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为了计算简单，我们只保留在数据集中至少出现5次的词\n",
    "counter = collections.Counter([tk for st in raw_dataset for tk in st])\n",
    "counter = dict(filter(lambda x: x[1] >= 5, counter.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# tokens: 887100'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将词映射到整数索引\n",
    "idx_to_token = [tk for tk,_ in counter.items()]\n",
    "token_to_idx = {tk: idx for idx, tk in enumerate(idx_to_token)}\n",
    "dataset = [[token_to_idx[tk] for tk in st if tk in token_to_idx]\n",
    "          for st in raw_dataset]\n",
    "num_tokens = sum([len(st) for st in dataset])\n",
    "'# tokens: %d' % num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二次采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# tokens: 375787'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def discard(idx):\n",
    "    return random.uniform(0, 1) < 1 - math.sqrt(\n",
    "    1e-4 / counter[idx_to_token[idx]] * num_tokens)\n",
    "\n",
    "subsampled_dataset = [[tk for tk in st if not discard(tk)] for st in dataset]\n",
    "'# tokens: %d' % sum([len(st) for st in subsampled_dataset]) #可以看到二次采样后去掉了一半左右的词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# the: before = 50770, after = 2162\n",
      "# join: before = 45, after = 45\n"
     ]
    }
   ],
   "source": [
    "#下面比较the join在二次采样前后出现在数据集中的次数\n",
    "def compare_counts(token):\n",
    "    return '# %s: before = %d, after = %d' % (token, sum(\n",
    "    [st.count(token_to_idx[token]) for st in dataset]), sum(\n",
    "    [st.count(token_to_idx[token]) for st in subsampled_dataset]))\n",
    "print(compare_counts('the'))\n",
    "print(compare_counts('join'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提取中心词和背景词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将与中心词距离不超过背景窗口大小的词作为它的背景词。\n",
    "#每次在整数1和max_window_size（最大背景窗口）之间随即均匀采样一个整数作为背景窗口的大小\n",
    "\n",
    "def get_centers_and_contexts(dataset, max_window_size):\n",
    "    centers, contexts = [], []\n",
    "    for st in dataset:\n",
    "        if len(st) < 2: #每个句子至少要有两个词才可能组成一对“中心词-背景词”\n",
    "            continue\n",
    "        centers += st\n",
    "        for center_i in range(len(st)):\n",
    "            window_size = random.randint(1, max_window_size)\n",
    "            indices = list(range(max(0, center_i - window_size),\n",
    "                                min(len(st), center_i + 1 + window_size)))\n",
    "\n",
    "            indices.remove(center_i) #将中心词排除在背景词之外\n",
    "            contexts.append([st[idx] for idx in indices])\n",
    "    return centers, contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset [[0, 1, 2, 3, 4, 5, 6], [7, 8, 9]]\n",
      "center 0 has contexts [1, 2]\n",
      "center 1 has contexts [0, 2]\n",
      "center 2 has contexts [0, 1, 3, 4]\n",
      "center 3 has contexts [1, 2, 4, 5]\n",
      "center 4 has contexts [3, 5]\n",
      "center 5 has contexts [4, 6]\n",
      "center 6 has contexts [5]\n",
      "center 7 has contexts [8]\n",
      "center 8 has contexts [7, 9]\n",
      "center 9 has contexts [8]\n"
     ]
    }
   ],
   "source": [
    "#下面创建一个人工数据集， 其中含有词数分别为7和3的两个句子。设最大背景窗口为2，打印所有中心词和它们的背景词\n",
    "tiny_dataset = [list(range(7)), list(range(7,10))]\n",
    "print('dataset', tiny_dataset)\n",
    "for center, context in zip(*get_centers_and_contexts(tiny_dataset, 2)):\n",
    "    print('center', center, 'has contexts', context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实验中我们设最大背景窗口大小为5.下面提取数据集中所有的中心词及其背景词\n",
    "all_centers, all_contexts = get_centers_and_contexts(subsampled_dataset, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 负采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于一对中心词和背景词，我们随机采样K个噪声词（实验中设置K=5）。根据论文建议，噪声词采样概率P(w)设为w词频与总词频之比的0.75次方\n",
    "\n",
    "def get_negatives(all_contexts, sampling_weights, K):\n",
    "    all_negatives, neg_candidates, i = [], [], 0\n",
    "    population = list(range(len(sampling_weights)))\n",
    "    for contexts in all_contexts:\n",
    "        negatives = []\n",
    "        while len(negatives) < len(contexts) * K:\n",
    "            if i == len(neg_candidates):\n",
    "                #根据每个词的权重（sampling_weights）随机生成k个词的索引作为噪声词\n",
    "                #为了高效计算，可以将k设的稍微大一点\n",
    "                i, neg_candidates = 0, random.choices(population, sampling_weights, k=int(1e5))\n",
    "            neg, i = neg_candidates[i], i + 1\n",
    "            #噪声词不能是背景词\n",
    "            if neg not in set(context):\n",
    "                negatives.append(neg)\n",
    "        all_negatives.append(negatives)\n",
    "            \n",
    "    return all_negatives\n",
    "\n",
    "sampling_weights = [counter[w] ** 0.75 for w in idx_to_token]\n",
    "all_negatives = get_negatives(all_contexts, sampling_weights, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从数据集中提取所有中心词all_centers,以及每个中心词对应的背景词all_contexts和噪声词all_negatives。先定义一个Dataset类\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, centers, contexts, negatives):\n",
    "        assert len(centers) == len(contexts) == len(negatives)\n",
    "        self.centers = centers\n",
    "        self.contexts = contexts\n",
    "        self.negatives = negatives\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (self.centers[index], self.contexts[index], self.negatives[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 小批量读取\n",
    "\n",
    "def batchify(data):\n",
    "    \n",
    "    max_len = max(len(c) + len(n) for _,c,n in data)\n",
    "    centers, contexts_negatives, masks, labels = [], [], [], []\n",
    "    for center, context, negative in data:\n",
    "        cur_len = len(context) + len(negative)\n",
    "        centers += [center]\n",
    "        contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
    "        masks += [[1] * cur_len + [0] * (max_len - cur_len)]\n",
    "        labels += [[1] * len(context) + [0] * (max_len - len(context))]\n",
    "    \n",
    "    return (torch.tensor(centers).view(-1,1), torch.tensor(contexts_negatives), torch.tensor(masks), torch.tensor(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers shape: torch.Size([512, 1])\n",
      "contexts_negatives shape: torch.Size([512, 60])\n",
      "masks shape: torch.Size([512, 60])\n",
      "labels shape: torch.Size([512, 60])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "num_workers = 0 if sys.platform.startswith('win32') else 4\n",
    "\n",
    "dataset = MyDataset(all_centers, all_contexts, all_negatives)\n",
    "data_iter = Data.DataLoader(dataset, batch_size, shuffle=True,\n",
    "                           collate_fn = batchify, num_workers=num_workers)\n",
    "\n",
    "for batch in data_iter:\n",
    "    for name,data in zip(['centers', 'contexts_negatives', 'masks', 'labels'], batch):\n",
    "        print(name, 'shape:', data.shape)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过使用嵌入层和小批量乘法来实现跳字模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  嵌入层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取词嵌入的层称为嵌入层，在pytorch中可以通过创建nn.Embedding实例得到。嵌入层的权重是一个矩阵，其行数为词典大小（num_embedding），列数为每个词向量的维度（embedding_dim）."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 8.2566e-01, -4.1418e-01,  1.0827e+00,  3.8593e-01],\n",
       "        [ 1.0893e+00,  6.8264e-01, -6.2649e-01, -2.7596e-01],\n",
       "        [-4.7299e-01,  4.5971e-01,  5.7199e-01, -6.5727e-01],\n",
       "        [-2.6608e-01,  4.8379e-01, -2.0680e-01,  8.4419e-01],\n",
       "        [ 3.4442e-01,  3.2321e-01,  1.9970e+00, -6.3670e-01],\n",
       "        [ 8.4116e-01,  8.2674e-01,  6.8717e-01, -3.6522e+00],\n",
       "        [ 7.8292e-01,  1.1777e+00,  1.5296e+00, -2.8856e-01],\n",
       "        [ 4.5455e-01,  1.1677e+00,  1.3419e+00,  1.8962e+00],\n",
       "        [ 1.1185e+00,  5.8836e-01, -1.3153e-01,  2.6126e-01],\n",
       "        [ 2.0977e-03, -6.4240e-01,  1.8078e+00, -1.7651e+00],\n",
       "        [ 9.6602e-01, -7.5056e-01,  1.3155e+00, -2.5733e-01],\n",
       "        [ 5.2213e-01, -1.5378e+00, -3.1501e+00,  9.6825e-02],\n",
       "        [ 4.8643e-01,  2.9936e-01, -9.8870e-01,  1.7854e+00],\n",
       "        [ 6.3611e-02,  1.1025e+00, -1.9858e-02, -7.3321e-01],\n",
       "        [-5.6354e-01, -1.6684e-01, -6.1249e-02,  7.2817e-01],\n",
       "        [ 5.5789e-01, -5.1981e-01,  5.3205e-01,  2.9197e-01],\n",
       "        [-4.9638e-01, -1.4801e-01,  4.4071e-01,  3.5107e-01],\n",
       "        [ 2.0496e+00, -1.3893e+00,  6.6412e-01,  3.9858e-01],\n",
       "        [ 1.5987e-01, -1.5185e+00,  4.8596e-01,  3.3034e-01],\n",
       "        [ 1.8158e+00,  1.0626e+00,  8.2213e-01, -6.3219e-01]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = nn.Embedding(num_embeddings=20, embedding_dim=4)\n",
    "embed.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0893,  0.6826, -0.6265, -0.2760],\n",
       "         [-0.4730,  0.4597,  0.5720, -0.6573],\n",
       "         [-0.2661,  0.4838, -0.2068,  0.8442]],\n",
       "\n",
       "        [[ 0.3444,  0.3232,  1.9970, -0.6367],\n",
       "         [ 0.8412,  0.8267,  0.6872, -3.6522],\n",
       "         [ 0.7829,  1.1777,  1.5296, -0.2886]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3],[4, 5, 6]], dtype=torch.long)\n",
    "embed(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  小批量乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 6])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bmm对两个小批量中的矩阵一一做乘法\n",
    "X = torch.ones((2, 1, 4))\n",
    "Y = torch.ones((2, 4, 6))\n",
    "torch.bmm(X, Y).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 跳字模型的前向计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在前向计算中，跳字模型的输入包括包含中心词索引center以及连结的背景词与噪声词索引contexts_and_negatives.其中center变量的形状为（批量大小，1），而contexts_and_negatives的形状为（批量大小， max_len）。这两个变量先通过词嵌入层分别由词索引变换为词向量，再通过小批量乘法得到形状为（批量大小，1，max_len）的输出。输出中的每个元素是中心词向量与背景词向量或噪声词向量的内积。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_gram(center, contexts_and_negatives, embed_v, embed_u):\n",
    "    v = embed_v(center)\n",
    "    u = embed_u(contexts_and_negatives)\n",
    "    pred = torch.bmm(v, u.permute(0, 2, 1))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二元交叉熵损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmodBinaryCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SigmodBinaryCrossEntropyLoss, self).__init__()\n",
    "    def forward(self, inputs, targets, mask=None):\n",
    "        inputs, targets, mask = inputs.float(), targets.float(), mask.float()\n",
    "        res = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\", weight=mask)\n",
    "        return res.mean(dim=1)\n",
    "\n",
    "loss = SigmodBinaryCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以通过掩码变量指定小批量中参与损失函数计算的部分预测值和标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8740, 1.2100])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = torch.tensor([[1.5, 0.3, -1, 2], [1.1, -0.6, 2.2, 0.4]])\n",
    "label = torch.tensor([[1, 0, 0, 0], [1, 1, 0, 0]])#1/0 代表背景词/噪声词\n",
    "mask = torch.tensor([[1, 1, 1, 1], [1, 1, 1, 0]])#掩码变量\n",
    "loss(pred, label, mask) * mask.shape[1] / mask.float().sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8740\n",
      "1.2100\n"
     ]
    }
   ],
   "source": [
    "def sigmd(x):\n",
    "    return - math.log(1 / (1 + math.exp(-x)))\n",
    "print('%.4f' % ((sigmd(1.5) + sigmd(-0.3) + sigmd(1) + sigmd(-2)) /4)) # 注意1-sigmoid(x) = sigmoid(-x)\n",
    "print('%.4f' % ((sigmd(1.1) + sigmd(-0.6) + sigmd(-2.2)) / 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 100\n",
    "net = nn.Sequential(\n",
    "    nn.Embedding(num_embeddings=len(idx_to_token), embedding_dim=embed_size),\n",
    "    nn.Embedding(num_embeddings=len(idx_to_token), embedding_dim=embed_size)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, lr, num_epochs):\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"train on\", device)\n",
    "    net = net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start, l_sum, n = time.time(), 0.0, 0\n",
    "        \n",
    "        for batch in data_iter:\n",
    "            center, context_negaive, mask, label = [d.to(device) for d in batch]\n",
    "            pred = skip_gram(center, context_negaive, net[0], net[1])\n",
    "            l = (loss(pred.view(label.shape), label, mask) * mask.shape[1] / mask.float().sum(dim=1)).mean()\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            l_sum += l.cpu().item()\n",
    "            n += 1\n",
    "        print('epoch %d, loss %.2f, time %.2fs'\n",
    "              % (epoch + 1, l_sum / n, time.time() - start))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on cpu\n",
      "epoch 1, loss 1.95, time 121.09s\n",
      "epoch 2, loss 0.62, time 113.57s\n",
      "epoch 3, loss 0.45, time 113.87s\n",
      "epoch 4, loss 0.39, time 122.97s\n",
      "epoch 5, loss 0.37, time 136.42s\n",
      "epoch 6, loss 0.35, time 133.66s\n",
      "epoch 7, loss 0.34, time 148.57s\n",
      "epoch 8, loss 0.33, time 148.22s\n",
      "epoch 9, loss 0.32, time 124.64s\n",
      "epoch 10, loss 0.32, time 158.88s\n"
     ]
    }
   ],
   "source": [
    "train(net, 0.01, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 应用词嵌入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_tokens(query_token, k, embed):\n",
    "    W = embed.weight.data\n",
    "    x = W[token_to_idx[query_token]]\n",
    "    cos = torch.matmul(W, x) / (torch.sum(W * W, dim=1) * torch.sum(x * x) + 1e-9).sqrt()\n",
    "    _, topk = torch.topk(cos, k=k+1)\n",
    "    topk = topk.cpu().numpy()\n",
    "    for i in topk[1:]:\n",
    "        print('cosine sim=%.3f: %s' % (cos[i], (idx_to_token[i])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim=0.450: intel\n",
      "cosine sim=0.413: chicago-based\n",
      "cosine sim=0.410: edison\n"
     ]
    }
   ],
   "source": [
    "get_similar_tokens('chip', 3, net[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
